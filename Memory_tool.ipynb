{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bab08c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import tiktoken\n",
    "import openai\n",
    "import os\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36c286e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/Users/nicholasgannon/.cache/huggingface/datasets/Nickgannon10___json/Nickgannon10--langchain-docs-8c3c47a09abfa9a4/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92e5897c3119-0</td>\n",
       "      <td>.ipynb\\n.pdf\\nModel Comparison\\nModel Comparis...</td>\n",
       "      <td>https://python.langchain.com/en/latest/model_l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92e5897c3119-1</td>\n",
       "      <td>pink\\nprompt = PromptTemplate(template=\"What i...</td>\n",
       "      <td>https://python.langchain.com/en/latest/model_l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92e5897c3119-2</td>\n",
       "      <td>names = [str(open_ai_llm), str(cohere_llm)]\\nm...</td>\n",
       "      <td>https://python.langchain.com/en/latest/model_l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>92e5897c3119-3</td>\n",
       "      <td>So the final answer is:\\nCarlos Alcaraz\\nBy Ha...</td>\n",
       "      <td>https://python.langchain.com/en/latest/model_l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7a20e8db90b3-0</td>\n",
       "      <td>.rst\\n.pdf\\nLangChain Gallery\\n Contents \\nOpe...</td>\n",
       "      <td>https://python.langchain.com/en/latest/gallery...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                                               text  \\\n",
       "0  92e5897c3119-0  .ipynb\\n.pdf\\nModel Comparison\\nModel Comparis...   \n",
       "1  92e5897c3119-1  pink\\nprompt = PromptTemplate(template=\"What i...   \n",
       "2  92e5897c3119-2  names = [str(open_ai_llm), str(cohere_llm)]\\nm...   \n",
       "3  92e5897c3119-3  So the final answer is:\\nCarlos Alcaraz\\nBy Ha...   \n",
       "4  7a20e8db90b3-0  .rst\\n.pdf\\nLangChain Gallery\\n Contents \\nOpe...   \n",
       "\n",
       "                                              source  \n",
       "0  https://python.langchain.com/en/latest/model_l...  \n",
       "1  https://python.langchain.com/en/latest/model_l...  \n",
       "2  https://python.langchain.com/en/latest/model_l...  \n",
       "3  https://python.langchain.com/en/latest/model_l...  \n",
       "4  https://python.langchain.com/en/latest/gallery...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Nickgannon10/langchain-docs\", split=\"train\")\n",
    "\n",
    "data = dataset.to_pandas()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e9ee95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "model_name = 'text-embedding-ada-002'\n",
    "\n",
    "embed = OpenAIEmbeddings(\n",
    "    model=model_name,\n",
    "    openai_api_key=openai.api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ea9e2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "\n",
    "# find API key in console at app.pinecone.io\n",
    "YOUR_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "# find ENV (cloud region) next to API key in console\n",
    "YOUR_ENV = os.getenv(\"ENVIRONMENT\")\n",
    "\n",
    "index_name = 'langchain-retrieval-agent'\n",
    "pinecone.init(\n",
    "    api_key=YOUR_API_KEY,\n",
    "    environment=YOUR_ENV\n",
    ")\n",
    "\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    # we create a new index\n",
    "    pinecone.create_index(\n",
    "        name=index_name,\n",
    "        metric='dotproduct',\n",
    "        dimension=1536  \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cdadc8f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip3 install -U \"pinecone-client[grpc]\"\n",
    "index = pinecone.Index(index_name)\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fb1045e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [{\n",
    "    'id': doc['id'],\n",
    "    'text': doc['text'],\n",
    "    'metadata': {'url': doc['source']}\n",
    "} for doc in dataset]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "178e8e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2304"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6a3946d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f378f64d21a44dda28de3d813577041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from uuid import uuid4\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "for i in tqdm(range(0, len(documents), batch_size)):\n",
    "    # get end of batch\n",
    "    i_end = min(len(documents), i+batch_size)\n",
    "    batch = documents[i:i_end]\n",
    "    # first get metadata fields for this record\n",
    "    metadatas = [{**doc['metadata'], 'text': doc['text']} for doc in batch]\n",
    "    # get the list of text / documents\n",
    "    texts = [doc['text'] for doc in batch]\n",
    "    # create document embeddings\n",
    "    embeds = embed.embed_documents(texts)\n",
    "    # get IDs\n",
    "    ids = [doc['id'] for doc in batch]\n",
    "    # add everything to pinecone\n",
    "    index.upsert(vectors=zip(ids, embeds, metadatas))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6eda9deb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 2304}},\n",
       " 'total_vector_count': 2304}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "80afca23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "text_field = \"text\"\n",
    "\n",
    "# switch back to normal index for langchain\n",
    "index = pinecone.Index(index_name)\n",
    "\n",
    "vectorstore = Pinecone(\n",
    "    index, embed.embed_query, text_field\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "669adb80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='.rst\\n.pdf\\nChains\\nChains#\\nNote\\nConceptual Guide\\nUsing an LLM in isolation is fine for some simple applications,\\nbut many more complex ones require chaining LLMs - either with each other or with other experts.\\nLangChain provides a standard interface for Chains, as well as some common implementations of chains for ease of use.\\nThe following sections of documentation are provided:\\nGetting Started: A getting started guide for chains, to get you up and running quickly.\\nHow-To Guides: A collection of how-to guides. These highlight how to use various types of chains.\\nReference: API reference documentation for all Chain classes.\\nprevious\\nHow to use multiple memroy classes in the same chain\\nnext\\nGetting Started\\nBy Harrison Chase\\n    \\n      Â© Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Mar 28, 2023.', metadata={'url': 'https://python.langchain.com/en/latest/modules/chains.html'}),\n",
       " Document(page_content='.rst\\n.pdf\\nLLMs\\nLLMs#\\nNote\\nConceptual Guide\\nLarge Language Models (LLMs) are a core component of LangChain.\\nLangChain is not a provider of LLMs, but rather provides a standard interface through which\\nyou can interact with a variety of LLMs.\\nThe following sections of documentation are provided:\\nGetting Started: An overview of all the functionality the LangChain LLM class provides.\\nHow-To Guides: A collection of how-to guides. These highlight how to accomplish various objectives with our LLM class (streaming, async, etc).\\nIntegrations: A collection of examples on how to integrate different LLM providers with LangChain (OpenAI, Hugging Face, etc).\\nReference: API reference documentation for all LLM classes.\\nprevious\\nModels\\nnext\\nGetting Started\\nBy Harrison Chase\\n    \\n      Â© Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Mar 28, 2023.', metadata={'url': 'https://python.langchain.com/en/latest/modules/models/llms.html'}),\n",
       " Document(page_content='Started\\\\nModules\\\\nUse Cases\\\\nReference Docs\\\\nLangChain Ecosystem\\\\nAdditional Resources\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nWelcome to LangChain#\\\\nLarge language models (LLMs) are emerging as a transformative technology, enabling\\\\ndevelopers to build applications that they previously could not.\\\\nBut using these LLMs in isolation is often not enough to\\\\ncreate a truly powerful app - the real power comes when you are able to\\\\ncombine them with other sources of computation or knowledge.\\\\nThis library is aimed at assisting in the development of those types of applications. Common examples of these types of applications include:\\\\nâ“ Question Answering over specific documents\\\\n\\\\nDocumentation\\\\nEnd-to-end Example: Question Answering over Notion Database\\\\n\\\\nðŸ’¬ Chatbots\\\\n\\\\nDocumentation\\\\nEnd-to-end Example: Chat-LangChain\\\\n\\\\nðŸ¤– Agents\\\\n\\\\nDocumentation\\\\nEnd-to-end Example: GPT+WolframAlpha\\\\n\\\\n\\\\nGetting Started#\\\\nCheckout the below guide for a walkthrough of how to get started using LangChain to create an Language Model application.\\\\n\\\\nGetting Started Documentation\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nModules#\\\\nThere are several main modules that LangChain provides support for.\\\\nFor each module we provide some examples to', metadata={'url': 'https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/sitemap.html'})]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Tell me about the LLMChain documentation\"\n",
    "\n",
    "vectorstore.similarity_search(\n",
    "    query,  # our search query\n",
    "    k=3  # return 3 most relevant docs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1bf2e580",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# chat completion llm\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=openai.api_key,\n",
    "    model_name='gpt-3.5-turbo',\n",
    "    temperature=0.0\n",
    ")\n",
    "# conversational memory\n",
    "conversational_memory = ConversationBufferWindowMemory(\n",
    "    memory_key='chat_history',\n",
    "    k=5,\n",
    "    return_messages=True\n",
    ")\n",
    "# retrieval qa chain\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e055d46a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The LLMChain documentation provides information on how to create a chain in LangChain, which is made up of links that can be either primitives like LLMs or other chains. The most core type of chain is an LLMChain, which consists of a PromptTemplate and an LLM. The documentation includes code examples that demonstrate how to create a simple LLMChain that takes user input, formats it with a PromptTemplate, and then passes the formatted response to an LLM. The documentation also covers more complex chains and how to use them.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "65e9c997",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name='Knowledge Base',\n",
    "        func=qa.run,\n",
    "        description=(\n",
    "            'use this tool when answering general knowledge queries about langchain documentation '\n",
    "            'more information about the topic'\n",
    "        )\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dbec5628",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent\n",
    "\n",
    "agent = initialize_agent(\n",
    "    agent='chat-conversational-react-description',\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    max_iterations=3,\n",
    "    early_stopping_method='generate',\n",
    "    memory=conversational_memory\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2f523de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"Knowledge Base\",\n",
      "    \"action_input\": \"LLMChain documentation\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mThe LangChain provides a standard interface for Large Language Models (LLMs) through which you can interact with a variety of LLMs. The LLMChain documentation includes the following sections:\n",
      "\n",
      "1. Getting Started: An overview of all the functionality the LangChain LLM class provides.\n",
      "2. How-To Guides: A collection of how-to guides. These highlight how to accomplish various objectives with our LLM class (streaming, async, etc).\n",
      "3. Integrations: A collection of examples on how to integrate different LLM providers with LangChain (OpenAI, Hugging Face, etc).\n",
      "4. Reference: API reference documentation for all LLM classes.\n",
      "\n",
      "You can find more information about LLMChain in the LangChain documentation.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"The LangChain provides a standard interface for Large Language Models (LLMs) through which you can interact with a variety of LLMs. The LLMChain documentation includes sections such as Getting Started, How-To Guides, Integrations, and Reference. You can find more information about LLMChain in the LangChain documentation.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Tell me about the LLMChain documentation',\n",
       " 'chat_history': [HumanMessage(content='Tell me about the LLMChain documentation', additional_kwargs={}),\n",
       "  AIMessage(content='The LangChain LLM class provides a standard interface through which you can interact with a variety of Large Language Models (LLMs). The LLMChain documentation includes sections such as Getting Started, How-To Guides, Integrations, and Reference. You can find more information about LLMChain in the LangChain documentation.', additional_kwargs={})],\n",
       " 'output': 'The LangChain provides a standard interface for Large Language Models (LLMs) through which you can interact with a variety of LLMs. The LLMChain documentation includes sections such as Getting Started, How-To Guides, Integrations, and Reference. You can find more information about LLMChain in the LangChain documentation.'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Tell me about the LLMChain documentation\"\n",
    "agent(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac223696",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
